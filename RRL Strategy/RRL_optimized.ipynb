{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based off of RRL_parallelized.ipynb\n",
    "##### Implements parallelization and TF graph execution with @tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define da model\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "INPUT_SIZE = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "input = layers.Input(shape=(INPUT_SIZE,))\n",
    "x = layers.Dense(20, activation='relu')(input)\n",
    "x = layers.Dense(10, activation='relu')(input)\n",
    "x = layers.Dense(1, activation='tanh')(x)\n",
    "F_curr_model = keras.Model(inputs=input, outputs=x)\n",
    "F_prev_model = keras.Model(inputs=input, outputs=x)\n",
    "F_curr_model.summary()\n",
    "\n",
    "F_curr_model.compile(optimizer=keras.optimizers.Adam(LEARNING_RATE), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following 2 functions take whole arrays, of shape (instances, size)\n",
    "\n",
    "def gen_price_series(size=10000, k=3, a=0.9, instances=1):\n",
    "    p_series = np.ndarray((instances, size,), dtype=np.float32)\n",
    "    b_series = np.ndarray((instances, size,), dtype=np.float32)\n",
    "    p_series[:, 0] = 0\n",
    "    b_series[:, 0] = 0\n",
    "\n",
    "    for i in range(1, size):\n",
    "        p_series[:, i] =  p_series[:, i-1] + b_series[:, i-1] + k * np.random.normal(size=instances)\n",
    "        b_series[:, i] = a * b_series[:, i-1] + np.random.normal(size=instances)\n",
    "\n",
    "    # shape: (instances)\n",
    "    R = np.max(p_series, axis=1) - np.min(p_series, axis=1)\n",
    "    z_series = np.exp(p_series / np.repeat(R[:, np.newaxis], size, axis=1))\n",
    "\n",
    "    return z_series\n",
    "\n",
    "def calc_price_returns(zt):\n",
    "    # returns the set rt, with the first element (0) being NaN\n",
    "    rt = np.ndarray(zt.shape, dtype=np.float32)\n",
    "    rt[:, 0] = np.nan\n",
    "    rt[:, 1:] = zt[:, 1:] - zt[:, :-1]\n",
    "    return rt\n",
    "\n",
    "\n",
    "\n",
    "# Following 2 functions take slices of the arrays, of shape (instances,)\n",
    "\n",
    "def calc_return(mu, rt, Ft_curr, Ft_prev, rft=tf.constant(0.0), delta=tf.constant(0.0)):\n",
    "    '''Calculates the returns (Rt) on-line.'''\n",
    "\n",
    "    return mu * (rft + Ft_prev * (rt - rft) - delta * tf.math.abs(Ft_curr - Ft_prev))\n",
    "\n",
    "def calc_DSR(n, Rt, At_prev, Bt_prev):\n",
    "    '''Calculates the differential Sharpe ratios (DSR, Dt) on-line.'''\n",
    "\n",
    "    At_curr = At_prev + n * (Rt - At_prev)\n",
    "    Bt_curr = Bt_prev + n * (tf.math.square(Rt) - Bt_prev)\n",
    "\n",
    "    dDt_dRt = (Bt_prev - At_prev * Rt) / tf.math.pow((Bt_prev - tf.math.square(At_prev)), 1.5)\n",
    "\n",
    "    return dDt_dRt, At_curr, Bt_curr\n",
    "\n",
    "\n",
    "from math import prod\n",
    "# Repeat grades of shape (instances,) to shape (instances,) + grad.shape\n",
    "def reshape_grad(grad, shape):\n",
    "    # takes a tf.Tensor grad of shape (instances,) and the target shape (variables)\n",
    "    instances = grad.shape[0]\n",
    "    total_elements = prod(shape)\n",
    "    grad = tf.repeat(grad, int(total_elements / instances))\n",
    "    grad = tf.reshape(grad, shape)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate trading using the outputs of the model\n",
    "# Takes SLICES of the arrays, of shape (instances,)\n",
    "\n",
    "def test_performance(zt, Ft):\n",
    "    # zt: (instances, size)\n",
    "    # Ft: (instances, size)\n",
    "\n",
    "    instances = zt.shape[0]\n",
    "    size = zt.shape[1]\n",
    "    Ft = np.sign(Ft)\n",
    "\n",
    "    values = np.ones((instances, size))\n",
    "    owned = np.zeros((instances,))\n",
    "    money = np.ones((instances,))\n",
    "    values[:, 0] = money\n",
    "\n",
    "    values_ideal = np.ones((instances, size))\n",
    "    owned_ideal = np.zeros((instances,))\n",
    "    money_ideal = np.ones((instances,))\n",
    "    values_ideal[:, 0] = money_ideal\n",
    "\n",
    "    for t in range(INPUT_SIZE, size - 1):\n",
    "\n",
    "        # Model Ft\n",
    "\n",
    "        # buy if Ft 1, owned 0 --> owned 1\n",
    "        # sell if Ft -1, owned 1 --> owned 0\n",
    "\n",
    "        # hold if Ft 0, owned 0 or 1\n",
    "        # hold if Ft 1, owned 1\n",
    "        # hold if Ft -1, owned 0\n",
    "\n",
    "        # model\n",
    "        buy = np.clip(Ft[:, t] * (1 - owned), 0, 1) # 1 if BUY, 0 if not\n",
    "        sell = np.clip(-Ft[:, t] * owned, 0, 1) # 1 if SELL, 0 if not\n",
    "        decision = buy - sell # 1 if BUY, -1 if SELL, 0 if HOLD\n",
    "        owned = np.clip(owned + decision, 0, 1)\n",
    "        money -= decision * zt[:, t]\n",
    "        values[:, t] = money + owned * zt[:, t]\n",
    "\n",
    "        # ideal\n",
    "        deltas_ideal = np.sign(zt[:, t + 1] - zt[:, t])\n",
    "        buy_ideal = np.clip(deltas_ideal * (1 - owned_ideal), 0, 1)\n",
    "        sell_ideal = np.clip(-deltas_ideal * owned_ideal, 0, 1)\n",
    "        decision_ideal = buy_ideal - sell_ideal\n",
    "        owned_ideal = np.clip(owned_ideal + decision_ideal, 0, 1)\n",
    "        money_ideal -= decision_ideal * zt[:, t]\n",
    "        values_ideal[:, t] = money_ideal + owned_ideal * zt[:, t]\n",
    "\n",
    "    return (values[-1] / zt[-1], values_ideal[-1] / zt[-1]), (values, values_ideal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def calc_grads(zt_curr, zt_prev, rt, At, Bt, N, MU, F_curr_model, F_prev_model):\n",
    "    '''\n",
    "    zt_curr: zt[:, t - INPUT_SIZE + 1:t + 1]\n",
    "    zt_prev: zt[:, t - INPUT_SIZE:t]\n",
    "    rt: rt[:, t]\n",
    "    At: At[:, t-1]\n",
    "    Bt: Bt[:, t-1]\n",
    "    '''\n",
    "\n",
    "    INSTANCES = zt_curr.shape[0]\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        F_curr = tf.reshape(F_curr_model(zt_curr), (INSTANCES,))\n",
    "        F_prev = tf.reshape(F_prev_model(zt_prev), (INSTANCES,)) # On the first iteration, this does not exist and is not used.\n",
    "        \n",
    "\n",
    "        # if the first iteration, F(t-1) does not yet exist so no update can be made.\n",
    "        # F_prev_model.set_weights(F_curr_model.get_weights())\n",
    "        # calculate the gradient\n",
    "        Rt = calc_return(MU, rt, F_curr, F_prev)\n",
    "        dDt_dRt, At_new, Bt_new = calc_DSR(N, Rt, At, Bt)\n",
    "\n",
    "    # calculate derivatives.\n",
    "    dRt_dFcurr = tape.gradient(Rt, F_curr) # shape: (instances,)\n",
    "    dRt_dFprev = tape.gradient(Rt, F_prev) # shape: (instances,)\n",
    "    dFcurr_dThetacurr = tape.jacobian(F_curr, F_curr_model.trainable_variables) # shape: (instances, MODEL VAR SHAPE)\n",
    "    dFprev_dThetaprev = tape.jacobian(F_prev, F_prev_model.trainable_variables) # shape: (instances, MODEL VAR SHAPE)\n",
    "\n",
    "    return F_curr, At_new, Bt_new, dDt_dRt, dRt_dFcurr, dRt_dFprev, dFcurr_dThetacurr, dFprev_dThetaprev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATION VARS\n",
    "import time\n",
    "\n",
    "MU = tf.constant(3.0)\n",
    "N = tf.constant(0.01)\n",
    "RISKFREE_RETURN = 0\n",
    "TRANS_COST = 0.1\n",
    "SERIES_LENGTH = 1000\n",
    "TRADING_DELAY = 100\n",
    "K = 3\n",
    "A = 0.9\n",
    "\n",
    "EPISODES = 100\n",
    "INSTANCES = 1\n",
    "\n",
    "BASELINE_SERIES = gen_price_series(size=SERIES_LENGTH, k=K, a=A, instances=INSTANCES)\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "\n",
    "    # generate price series\n",
    "    zt = gen_price_series(size=SERIES_LENGTH, k=K, a=A, instances=INSTANCES)\n",
    "    rt = calc_price_returns(zt)\n",
    "    Ft = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "\n",
    "    # breaks if init at one; MUST INIT ZERO\n",
    "    At = np.zeros((INSTANCES, SERIES_LENGTH,), dtype=np.float32)\n",
    "    Bt = np.zeros((INSTANCES, SERIES_LENGTH,), dtype=np.float32)\n",
    "\n",
    "    SR_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "    DSR_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "    Rt_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "    dD_series = np.ones((INSTANCES, SERIES_LENGTH,))\n",
    "    autodiff_series = np.ones((INSTANCES, SERIES_LENGTH,))\n",
    "\n",
    "\n",
    "    # plt.plot(zt)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    for t in range(INPUT_SIZE, SERIES_LENGTH): \n",
    "        Ft[:, t], At[:, t], Bt[:, t], dDt_dRt, dRt_dFcurr, dRt_dFprev, dFcurr_dThetacurr, dFprev_dThetaprev = calc_grads(zt[:, t - INPUT_SIZE + 1:t + 1], zt[:, t - INPUT_SIZE:t], rt[:, t], At[:, t-1], Bt[:, t-1], N, MU, F_curr_model, F_prev_model)\n",
    "\n",
    "        # dont update parameters if DSR hasn't stabilized yet\n",
    "        if t < TRADING_DELAY:\n",
    "            continue\n",
    "\n",
    "        # Set F(t-1) to F(t) for the next iteration.\n",
    "        F_prev_model.set_weights(F_curr_model.get_weights())\n",
    "\n",
    "        # print(len(F_curr_model.trainable_variables))\n",
    "        \n",
    "        if t != INPUT_SIZE:\n",
    "            # multiply derivatives together.\n",
    "            gradient_update = []\n",
    "\n",
    "            for i in range(len(dFcurr_dThetacurr)):\n",
    "                total_elements = prod(dFcurr_dThetacurr[i].shape)\n",
    "\n",
    "                dDt_dRt_exp = reshape_grad(dDt_dRt, dFcurr_dThetacurr[i].shape)\n",
    "                dRt_dFcurr_exp = reshape_grad(dRt_dFcurr, dFcurr_dThetacurr[i].shape)\n",
    "                dRt_dFprev_exp = reshape_grad(dRt_dFprev, dFcurr_dThetacurr[i].shape)\n",
    "\n",
    "                grad = dDt_dRt_exp * (dRt_dFcurr_exp * dFcurr_dThetacurr[i] + dRt_dFprev_exp * dFprev_dThetaprev[i])\n",
    "                grad = tf.reduce_sum(grad, axis=0)\n",
    "                grad *= LEARNING_RATE / INSTANCES # divide by instances since the gradients are summed over all instances.\n",
    "\n",
    "                # gradient_update.append(tf.reshape(grad, F_curr_model.trainable_variables[i].shape))\n",
    "                gradient_update.append(grad)\n",
    "\n",
    "            vars = F_curr_model.trainable_variables\n",
    "            for i in range(len(vars)):\n",
    "                vars[i].assign_add(gradient_update[i])\n",
    "\n",
    "    if ep % 1 == 0:\n",
    "        print(\"Episode: \", ep)\n",
    "        # test performance\n",
    "        deltas, val_series = test_performance(zt, Ft)\n",
    "\n",
    "        fig, ax = plt.subplots(2, figsize=(5, 3))\n",
    "\n",
    "        ax[0].plot(zt[0])\n",
    "        ax[0].plot(val_series[0][0])\n",
    "        ax[0].set_title(\"price\")\n",
    "\n",
    "        ax[1].plot(Ft[0] * 0)\n",
    "        ax[1].plot(Ft[0])\n",
    "        ax[1].set_title(\"decision\")\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
