{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based off RRL_training.ipynb\n",
    "##### Parallelizes training over multiple time series at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                110       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define da model\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "INPUT_SIZE = 10\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "input = layers.Input(shape=(INPUT_SIZE,))\n",
    "x = layers.Dense(10, activation='relu')(input)\n",
    "x = layers.Dense(1, activation='tanh')(x)\n",
    "F_curr_model = keras.Model(inputs=input, outputs=x)\n",
    "F_prev_model = keras.Model(inputs=input, outputs=x)\n",
    "F_curr_model.summary()\n",
    "\n",
    "F_curr_model.compile(optimizer=keras.optimizers.Adam(LEARNING_RATE), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following 2 functions take whole arrays, of shape (instances, size)\n",
    "\n",
    "def gen_price_series(size=10000, k=3, a=0.9, instances=1):\n",
    "    p_series = np.ndarray((instances, size,))\n",
    "    b_series = np.ndarray((instances, size,))\n",
    "    p_series[:, 0] = 0\n",
    "    b_series[:, 0] = 0\n",
    "\n",
    "    for i in range(1, size):\n",
    "        p_series[:, i] =  p_series[:, i-1] + b_series[:, i-1] + k * np.random.normal(size=instances)\n",
    "        b_series[:, i] = a * b_series[:, i-1] + np.random.normal(size=instances)\n",
    "\n",
    "    # shape: (instances)\n",
    "    R = np.max(p_series, axis=1) - np.min(p_series, axis=1)\n",
    "    z_series = np.exp(p_series / np.repeat(R[:, np.newaxis], size, axis=1))\n",
    "\n",
    "    return z_series\n",
    "\n",
    "def calc_price_returns(zt):\n",
    "    # returns the set rt, with the first element (0) being NaN\n",
    "    rt = np.ndarray(zt.shape)\n",
    "    rt[:, 0] = np.nan\n",
    "    rt[:, 1:] = zt[:, 1:] - zt[:, :-1]\n",
    "    return rt\n",
    "\n",
    "\n",
    "\n",
    "# Following 2 functions take slices of the arrays, of shape (instances,)\n",
    "\n",
    "def calc_return(mu, rt, Ft_curr, Ft_prev, rft=0, delta=0):\n",
    "    '''Calculates the returns (Rt) on-line.'''\n",
    "\n",
    "    return mu * (rft + Ft_prev * (rt - rft) - delta * tf.math.abs(Ft_curr - Ft_prev))\n",
    "\n",
    "def calc_DSR(n, Rt, At_prev, Bt_prev):\n",
    "    '''Calculates the differential Sharpe ratios (DSR, Dt) on-line.'''\n",
    "\n",
    "    At_curr = At_prev + n * (Rt - At_prev)\n",
    "    Bt_curr = Bt_prev + n * (Rt ** 2 - Bt_prev)\n",
    "\n",
    "    dDt_dRt = (Bt_prev - At_prev * Rt) / ((Bt_prev - At_prev ** 2) ** 1.5)\n",
    "\n",
    "    return dDt_dRt, At_curr, Bt_curr\n",
    "\n",
    "\n",
    "from math import prod\n",
    "# Repeat grades of shape (instances,) to shape (instances,) + grad.shape\n",
    "def reshape_grad(grad, shape):\n",
    "    # takes a tf.Tensor grad of shape (instances,) and the target shape (variables)\n",
    "    instances = grad.shape[0]\n",
    "    total_elements = prod(shape)\n",
    "    grad = tf.repeat(grad, int(total_elements / instances))\n",
    "    grad = tf.reshape(grad, shape)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate trading using the outputs of the model\n",
    "# Takes SLICES of the arrays, of shape (instances,)\n",
    "\n",
    "def test_performance(zt, Ft):\n",
    "    # zt: (instances, size)\n",
    "    # Ft: (instances, size)\n",
    "\n",
    "    instances = zt.shape[0]\n",
    "    size = zt.shape[1]\n",
    "    Ft = np.sign(Ft)\n",
    "\n",
    "    values = np.ones((instances, size))\n",
    "    owned = np.zeros((instances,))\n",
    "    money = np.ones((instances,))\n",
    "    values[:, 0] = money\n",
    "\n",
    "    values_ideal = np.ones((instances, size))\n",
    "    owned_ideal = np.zeros((instances,))\n",
    "    money_ideal = np.ones((instances,))\n",
    "    values_ideal[:, 0] = money_ideal\n",
    "\n",
    "    for t in range(INPUT_SIZE, size - 1):\n",
    "\n",
    "        # Model Ft\n",
    "\n",
    "        # buy if Ft 1, owned 0 --> owned 1\n",
    "        # sell if Ft -1, owned 1 --> owned 0\n",
    "\n",
    "        # hold if Ft 0, owned 0 or 1\n",
    "        # hold if Ft 1, owned 1\n",
    "        # hold if Ft -1, owned 0\n",
    "\n",
    "        # model\n",
    "        buy = np.clip(Ft[:, t] * (1 - owned), 0, 1) # 1 if BUY, 0 if not\n",
    "        sell = np.clip(-Ft[:, t] * owned, 0, 1) # 1 if SELL, 0 if not\n",
    "        decision = buy - sell # 1 if BUY, -1 if SELL, 0 if HOLD\n",
    "        owned = np.clip(owned + decision, 0, 1)\n",
    "        money -= decision * zt[:, t]\n",
    "        values[:, t] = money + owned * zt[:, t]\n",
    "\n",
    "        # ideal\n",
    "        deltas_ideal = np.sign(zt[:, t + 1] - zt[:, t])\n",
    "        buy_ideal = np.clip(deltas_ideal * (1 - owned_ideal), 0, 1)\n",
    "        sell_ideal = np.clip(-deltas_ideal * owned_ideal, 0, 1)\n",
    "        decision_ideal = buy_ideal - sell_ideal\n",
    "        owned_ideal = np.clip(owned_ideal + decision_ideal, 0, 1)\n",
    "        money_ideal -= decision_ideal * zt[:, t]\n",
    "        values_ideal[:, t] = money_ideal + owned_ideal * zt[:, t]\n",
    "\n",
    "    return (values[-1] / zt[-1], values_ideal[-1] / zt[-1]), (values, values_ideal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 0\n",
    "\n",
    "# zt = gen_price_series(size=50, instances=3)\n",
    "# _, val_series = test_performance(zt, np.random.rand(3, 50))\n",
    "# fig, ax = plt.subplots(3, 1)\n",
    "# print(val_series[1].shape)\n",
    "# print(val_series[1][:, -5:])\n",
    "# for i in range(3):\n",
    "#     ax[i].plot(zt[i])\n",
    "#     ax[i].plot(val_series[1][i])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ft = np.array((0, 1, 1, 1, -1, -1, 0, -1))\n",
    "# owned = np.array((1, 0, 0, 1, 1, 1, 0, 0))\n",
    "# money = np.zeros(8)\n",
    "# zt = np.arange(8)\n",
    "\n",
    "# # [HOLD,  BUY,  BUY, HOLD, SELL, SELL, HOLD, HOLD]\n",
    "# # [OWND, OWND, OWND, OWND, NONE, NONE, NONE, NONE]\n",
    "\n",
    "# buy = Ft * (1 - owned) # 1 if BUY, 0 if not\n",
    "# buy = np.clip(buy, 0, 1)\n",
    "# sell = -Ft * owned # 1 if SELL, 0 if not\n",
    "# sell = np.clip(sell, 0, 1)\n",
    "# decisions = buy - sell\n",
    "\n",
    "# print(\"own: \", owned)\n",
    "# print(\"buy: \", buy)\n",
    "# print(\"sell:\", sell)\n",
    "# owned += decisions\n",
    "# owned = np.clip(owned, 0, 1)\n",
    "# print(\"new: \", owned)\n",
    "# money -= zt * decisions\n",
    "# print(\"mony:\", money) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function jacobian at 0x000002667B6DFA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "TRACING JACOBIANS\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function jacobian at 0x000002667B6DFA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\AppData\\Local\\Temp\\ipykernel_8704\\2138195476.py:65: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  SR_series[:, t] = np.mean(Rt_series[:, INPUT_SIZE:t]) / np.std(Rt_series[:, INPUT_SIZE:t])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n",
      "TRACING JACOBIANS\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m Ft[:, t] \u001b[38;5;241m=\u001b[39m F_curr\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# if the first iteration, F(t-1) does not yet exist so no update can be made.\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m F_prev_model\u001b[38;5;241m.\u001b[39mset_weights(\u001b[43mF_curr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# calculate the gradient\u001b[39;00m\n\u001b[0;32m     61\u001b[0m Rt \u001b[38;5;241m=\u001b[39m calc_return(MU, rt[:, t], F_curr, F_prev)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2641\u001b[0m, in \u001b[0;36mModel.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the weights of the model.\u001b[39;00m\n\u001b[0;32m   2636\u001b[0m \n\u001b[0;32m   2637\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;124;03m    A flat list of Numpy arrays.\u001b[39;00m\n\u001b[0;32m   2639\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 2641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py:1828\u001b[0m, in \u001b[0;36mLayer.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1827\u001b[0m         output_weights\u001b[38;5;241m.\u001b[39mappend(weight)\n\u001b[1;32m-> 1828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:4240\u001b[0m, in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   4228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[0;32m   4229\u001b[0m \n\u001b[0;32m   4230\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4237\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[0;32m   4238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m-> 4240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m   4241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m   4242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:4240\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   4228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[0;32m   4229\u001b[0m \n\u001b[0;32m   4230\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4237\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[0;32m   4238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m-> 4240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m   4241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m   4242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:725\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 725\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:704\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    702\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    707\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    708\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[0;32m    710\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[0;32m    711\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    712\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:694\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    693\u001b[0m   gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mdisable_copy_on_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m--> 694\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:524\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    523\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    527\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# GENERATION VARS\n",
    "import time\n",
    "\n",
    "MU = 3\n",
    "N = 0.01\n",
    "RISKFREE_RETURN = 0\n",
    "TRANS_COST = 0.1\n",
    "SERIES_LENGTH = 1000\n",
    "TRADING_DELAY = 100\n",
    "K = 3\n",
    "A = 0.9\n",
    "\n",
    "EPISODES = 10\n",
    "INSTANCES = 1\n",
    "\n",
    "BASELINE_SERIES = gen_price_series(size=SERIES_LENGTH, k=K, a=A, instances=INSTANCES)\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "\n",
    "    # generate price series\n",
    "    zt = gen_price_series(size=SERIES_LENGTH, k=K, a=A, instances=INSTANCES)\n",
    "    rt = calc_price_returns(zt)\n",
    "    Ft = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "\n",
    "    # breaks if init at one; MUST INIT ZERO\n",
    "    At = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "    Bt = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "\n",
    "    SR_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "    DSR_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "    Rt_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "    dD_series = np.ones((INSTANCES, SERIES_LENGTH,))\n",
    "    autodiff_series = np.ones((INSTANCES, SERIES_LENGTH,))\n",
    "\n",
    "\n",
    "    # plt.plot(zt)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def jacobian(tape, F, F_vars):\n",
    "        print(\"TRACING JACOBIANS\")\n",
    "        dF_dVar = tape.jacobian(F, F_vars) # shape: (instances, MODEL VAR SHAPE)\n",
    "        return dF_dVar\n",
    "    \n",
    "    for t in range(INPUT_SIZE, SERIES_LENGTH):\n",
    "        Ta = time.time()\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # prediction, set new position\n",
    "\n",
    "            F_curr = tf.reshape(F_curr_model(zt[:, t - INPUT_SIZE + 1:t + 1]), (INSTANCES,))\n",
    "            F_prev = tf.reshape(F_prev_model(zt[:, t - INPUT_SIZE:t]), (INSTANCES,)) # On the first iteration, this does not exist and is not used.\n",
    "            \n",
    "            \n",
    "            Ft[:, t] = F_curr\n",
    "\n",
    "            # if the first iteration, F(t-1) does not yet exist so no update can be made.\n",
    "            F_prev_model.set_weights(F_curr_model.get_weights())\n",
    "            # calculate the gradient\n",
    "            Rt = calc_return(MU, rt[:, t], F_curr, F_prev)\n",
    "            dDt_dRt, At[:, t], Bt[:, t] = calc_DSR(N, Rt, At[:, t - 1], Bt[:, t - 1])\n",
    "            \n",
    "            Rt_series[:, t] = Rt\n",
    "            SR_series[:, t] = np.mean(Rt_series[:, INPUT_SIZE:t]) / np.std(Rt_series[:, INPUT_SIZE:t])\n",
    "            DSR = (Bt[:, t-1] * (Rt - At[:, t-1]) - (1/2) * (At[:, t-1] * (Rt ** 2 - Bt[:, t-1]))) / ((Bt[:, t-1] - At[:, t-1] ** 2) ** 1.5)\n",
    "            DSR_series[:, t] = (Bt[:, t-1] * (Rt - At[:, t-1]) - (1/2) * (At[:, t-1] * (Rt ** 2 - Bt[:, t-1]))) / ((Bt[:, t-1] - At[:, t-1] ** 2) ** 1.5)\n",
    "            dD_series[:, t] = dDt_dRt\n",
    "\n",
    "        # calculate derivatives.\n",
    "        dRt_dFcurr = tape.gradient(Rt, F_curr) # shape: (instances,)\n",
    "        dRt_dFprev = tape.gradient(Rt, F_prev) # shape: (instances,)\n",
    "\n",
    "\n",
    "\n",
    "        # Tgrad = time.time()\n",
    "\n",
    "        \n",
    "        # print(F_curr)\n",
    "\n",
    "        # dFcurr_dThetacurr = tape.jacobian(F_curr, F_curr_model.trainable_variables) # shape: (instances, MODEL VAR SHAPE)\n",
    "        # dFprev_dThetaprev = tape.jacobian(F_prev, F_prev_model.trainable_variables) # shape: (instances, MODEL VAR SHAPE)\n",
    "        # print(dFcurr_dThetacurr)\n",
    "        # dFcurr_dThetacurr = jacobians(F_curr, F_curr_model)\n",
    "        # dFprev_dThetaprev = jacobians(F_prev, F_prev_model)\n",
    "        dFcurr_dThetacurr = jacobian(tape, F_curr, F_curr_model.trainable_variables) # shape: (instances, MODEL VAR SHAPE)\n",
    "        dFprev_dThetaprev = jacobian(tape, F_prev, F_prev_model.trainable_variables) # shape: (instances, MODEL VAR SHAPE)\n",
    "        \n",
    "        # print(\"Time grad:\", time.time() - Tgrad)\n",
    "\n",
    "\n",
    "\n",
    "        autodiff_series[:, t] = tape.gradient(DSR, Rt)\n",
    "\n",
    "        # print(len(dRt_dFcurr))\n",
    "        # print(dRt_dFcurr.shape)\n",
    "        # print(len(dFcurr_dThetacurr))\n",
    "        # print(dFcurr_dThetacurr[0].shape)\n",
    "\n",
    "        # dont update parameters if DSR hasn't stabilized yet\n",
    "        if t < TRADING_DELAY:\n",
    "            continue\n",
    "\n",
    "        # Set F(t-1) to F(t) for the next iteration.\n",
    "        F_prev_model.set_weights(F_curr_model.get_weights())\n",
    "\n",
    "        # print(len(F_curr_model.trainable_variables))\n",
    "        \n",
    "        if t != INPUT_SIZE:\n",
    "            # multiply derivatives together.\n",
    "            gradient_update = []\n",
    "\n",
    "\n",
    "            for i in range(len(dFcurr_dThetacurr)):\n",
    "                total_elements = prod(dFcurr_dThetacurr[i].shape)\n",
    "                # dDt_dRt = dDt_dRt.numpy().repeat(total_elements).reshape((INSTANCES,) + dFcurr_dThetacurr[i].shape)\n",
    "                # dRt_dFcurr = dRt_dFcurr.numpy().repeat(total_elements).reshape((INSTANCES,) + dFcurr_dThetacurr[i].shape)\n",
    "                # dRt_dFprev = dRt_dFprev.numpy().repeat(total_elements).reshape((INSTANCES,) + dFcurr_dThetacurr[i].shape)\n",
    "\n",
    "                dDt_dRt_exp = reshape_grad(dDt_dRt, dFcurr_dThetacurr[i].shape)\n",
    "                dRt_dFcurr_exp = reshape_grad(dRt_dFcurr, dFcurr_dThetacurr[i].shape)\n",
    "                dRt_dFprev_exp = reshape_grad(dRt_dFprev, dFcurr_dThetacurr[i].shape)\n",
    "\n",
    "                grad = dDt_dRt_exp * (dRt_dFcurr_exp * dFcurr_dThetacurr[i] + dRt_dFprev_exp * dFprev_dThetaprev[i])\n",
    "                grad = tf.reduce_sum(grad, axis=0)\n",
    "                grad *= LEARNING_RATE\n",
    "\n",
    "                # gradient_update.append(tf.reshape(grad, F_curr_model.trainable_variables[i].shape))\n",
    "                gradient_update.append(grad)\n",
    "\n",
    "\n",
    "            VERBOSE = False\n",
    "            if VERBOSE:\n",
    "                X = -1\n",
    "                print(\"\\nSAAAAA========================\")\n",
    "                # a = F_curr_model.trainable_variables[X].numpy()\n",
    "                # print(\"bias variable:\", F_curr_model.trainable_variables[X])\n",
    "                # print(F_curr_model.trainable_variables[-2])\n",
    "                # print(\"grad update:\", gradient_update[X])\n",
    "\n",
    "                # print(\"grad update:\", gradient_update[X].numpy())\n",
    "                # print(\"grad update:\", gradient_update)\n",
    "            # if np.isnan(gradient_update[X].numpy()):\n",
    "            #     dasufgapg = 1\n",
    "\n",
    "            vars = F_curr_model.trainable_variables\n",
    "            for i in range(len(vars)):\n",
    "                vars[i].assign_add(gradient_update[i])\n",
    "\n",
    "            if VERBOSE:\n",
    "                # b = F_curr_model.trainable_variables[X].numpy()\n",
    "                # print(\"delta:\", b - a)\n",
    "                # print(\"new bias :\", F_curr_model.trainable_variables[X])\n",
    "                # print(\"output:\", F_curr)\n",
    "                print(\"BASELINE output:\", F_curr_model(BASELINE_SERIES[-INPUT_SIZE:].reshape(1, INPUT_SIZE)))\n",
    "\n",
    "        Tb = time.time()\n",
    "        print(\"Time:\", Tb - Ta)\n",
    "\n",
    "    if ep % 1 == 0:\n",
    "        print(\"Episode: \", ep)\n",
    "        # test performance\n",
    "        deltas, val_series = test_performance(zt, Ft)\n",
    "\n",
    "        fig, ax = plt.subplots(3, 2, figsize=(15, 6))\n",
    "\n",
    "        ax[0, 0].plot(zt[0])\n",
    "        ax[0, 0].plot(val_series[0][0])\n",
    "        ax[0, 0].set_title(\"price\")\n",
    "\n",
    "        ax[1, 0].plot(SR_series[0, :])\n",
    "        ax[1, 0].set_title(\"SR\")\n",
    "\n",
    "        ax[0, 1].plot(DSR_series[0, :])\n",
    "        ax[0, 1].set_title(\"DSR\")\n",
    "\n",
    "        ax[1, 1].plot(dD_series[0, :])\n",
    "        ax[1, 1].set_title(\"dD/dR\")\n",
    "        ax[2, 1].plot(autodiff_series[0, :])\n",
    "        ax[2, 1].set_title(\"autodiff_series\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # plt.plot(val_series[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
