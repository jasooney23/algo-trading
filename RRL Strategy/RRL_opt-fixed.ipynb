{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Based off of optimized.ipynb**\n",
    "Implements parallelization, graph execution, and fixed F(t-1) problem\n",
    "\n",
    "Currently uses `Strategy 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define da model\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "INPUT_SIZE = 30\n",
    "LEARNING_RATE = 0.00001\n",
    "TANH_CONSTANT = 0.01\n",
    "\n",
    "input = layers.Input(shape=(INPUT_SIZE,))\n",
    "# x = layers.Dense(30, activation='elu')(input)\n",
    "# x = layers.GRU(20, return_sequences=True)(input)\n",
    "# x = layers.GRU(20, return_sequences=False)(input)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(30, activation='elu')(input)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(20, activation='elu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(1, activation='linear')(x)\n",
    "x = layers.Add()([tf.math.tanh(x), x * tf.constant(TANH_CONSTANT, dtype=tf.float32)]) # This bit is the activation function: tanh(x) + x * TANH_CONSTANT\n",
    "F_model = keras.Model(inputs=input, outputs=x)\n",
    "F_model.summary()\n",
    "\n",
    "F_model.compile(optimizer=keras.optimizers.Adam(LEARNING_RATE), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following 2 functions take whole arrays, of shape (instances, size)\n",
    "\n",
    "def gen_price_series(size=10000, k=3, a=0.9, instances=1):\n",
    "    p_series = np.ndarray((instances, size,), dtype=np.float32)\n",
    "    b_series = np.ndarray((instances, size,), dtype=np.float32)\n",
    "    p_series[:, 0] = 0\n",
    "    b_series[:, 0] = 0\n",
    "\n",
    "    for i in range(1, size):\n",
    "        p_series[:, i] =  p_series[:, i-1] + b_series[:, i-1] + k * np.random.normal(size=instances)\n",
    "        b_series[:, i] = a * b_series[:, i-1] + np.random.normal(size=instances)\n",
    "\n",
    "    # shape: (instances)\n",
    "    R = np.max(p_series, axis=1) - np.min(p_series, axis=1)\n",
    "    z_series = np.exp(p_series / np.repeat(R[:, np.newaxis], size, axis=1))\n",
    "\n",
    "    return z_series\n",
    "\n",
    "def calc_price_returns(zt):\n",
    "    # returns the set rt, with the first element (0) being NaN\n",
    "    rt = np.ndarray(zt.shape, dtype=np.float32)\n",
    "    rt[:, 0] = np.nan\n",
    "    rt[:, 1:] = zt[:, 1:] - zt[:, :-1]\n",
    "    return rt\n",
    "\n",
    "\n",
    "\n",
    "# Following 2 functions take slices of the arrays, of shape (instances,)\n",
    "\n",
    "def calc_return(mu, rt, Ft_curr, Ft_prev, rft=tf.constant(0.0), delta=tf.constant(0.0)):\n",
    "    '''Calculates the returns (Rt) on-line.'''\n",
    "\n",
    "    return mu * (rft + Ft_prev * (rt - rft) - delta * tf.math.abs(Ft_curr - Ft_prev))\n",
    "\n",
    "def calc_DSR(n, Rt, At_prev, Bt_prev):\n",
    "    '''Calculates the differential Sharpe ratios (DSR, Dt) on-line.'''\n",
    "\n",
    "    At_curr = At_prev + n * (Rt - At_prev)\n",
    "    Bt_curr = Bt_prev + n * (tf.math.square(Rt) - Bt_prev)\n",
    "\n",
    "    dDt_dRt = (Bt_prev - At_prev * Rt) / tf.math.pow((Bt_prev - tf.math.square(At_prev)), 1.5)\n",
    "\n",
    "    return dDt_dRt, At_curr, Bt_curr\n",
    "\n",
    "\n",
    "from math import prod\n",
    "# Repeat grades of shape (instances,) to shape (instances,) + grad.shape\n",
    "def reshape_grad(grad, shape):\n",
    "    # takes a tf.Tensor grad of shape (instances,) and the target shape (variables)\n",
    "    instances = grad.shape[0]\n",
    "    total_elements = prod(shape)\n",
    "    grad = tf.repeat(grad, int(total_elements / instances))\n",
    "    grad = tf.reshape(grad, shape)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate trading using the outputs of the model\n",
    "# Takes SLICES of the arrays, of shape (instances,)\n",
    "\n",
    "def test_performance(zt, Ft):\n",
    "    # zt: (instances, size)\n",
    "    # Ft: (instances, size)\n",
    "\n",
    "    instances = zt.shape[0]\n",
    "    size = zt.shape[1]\n",
    "    Ft = np.sign(Ft)\n",
    "\n",
    "    values = np.ones((instances, size))\n",
    "    owned = np.zeros((instances,))\n",
    "    money = np.ones((instances,))\n",
    "    values[:, 0] = money\n",
    "\n",
    "    values_ideal = np.ones((instances, size))\n",
    "    owned_ideal = np.zeros((instances,))\n",
    "    money_ideal = np.ones((instances,))\n",
    "    values_ideal[:, 0] = money_ideal\n",
    "\n",
    "    for t in range(INPUT_SIZE, size - 1):\n",
    "\n",
    "        # Model Ft\n",
    "\n",
    "        # buy if Ft 1, owned 0 --> owned 1\n",
    "        # sell if Ft -1, owned 1 --> owned 0\n",
    "\n",
    "        # hold if Ft 0, owned 0 or 1\n",
    "        # hold if Ft 1, owned 1\n",
    "        # hold if Ft -1, owned 0\n",
    "\n",
    "        # model\n",
    "        buy = np.clip(Ft[:, t] * (1 - owned), 0, 1) # 1 if BUY, 0 if not\n",
    "        sell = np.clip(-Ft[:, t] * owned, 0, 1) # 1 if SELL, 0 if not\n",
    "        decision = buy - sell # 1 if BUY, -1 if SELL, 0 if HOLD\n",
    "        owned = np.clip(owned + decision, 0, 1)\n",
    "        money -= decision * zt[:, t]\n",
    "        values[:, t] = money + owned * zt[:, t]\n",
    "\n",
    "        # ideal\n",
    "        deltas_ideal = np.sign(zt[:, t + 1] - zt[:, t])\n",
    "        buy_ideal = np.clip(deltas_ideal * (1 - owned_ideal), 0, 1)\n",
    "        sell_ideal = np.clip(-deltas_ideal * owned_ideal, 0, 1)\n",
    "        decision_ideal = buy_ideal - sell_ideal\n",
    "        owned_ideal = np.clip(owned_ideal + decision_ideal, 0, 1)\n",
    "        money_ideal -= decision_ideal * zt[:, t]\n",
    "        values_ideal[:, t] = money_ideal + owned_ideal * zt[:, t]\n",
    "    \n",
    "    values[:, -1] = np.nan\n",
    "    values_ideal[:, -1] = np.nan\n",
    "\n",
    "    return (values[-1] / zt[-1], values_ideal[-1] / zt[-1]), (values, values_ideal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def calc_grads(zt, rt, At, Bt, N, MU, F_model, F_prev, rft=tf.constant(0.0), transaction_cost=tf.constant(0.0), random=tf.constant(1.0)):\n",
    "    '''\n",
    "    zt: zt[:, t - INPUT_SIZE + 1:t + 1]\n",
    "    rt: rt[:, t]\n",
    "    At: At[:, t-1]\n",
    "    Bt: Bt[:, t-1]\n",
    "    '''\n",
    "\n",
    "    # print(\"TRACING calc_grads()\")\n",
    "\n",
    "    INSTANCES = zt.shape[0]\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        F_curr = tf.reshape(F_model(zt), (INSTANCES,))\n",
    "        \n",
    "        # calculate the gradient\n",
    "        # Rt = calc_return(MU, rt, F_curr, F_prev, rft, transaction_cost)\n",
    "\n",
    "        # THIS WORKS\n",
    "        # Rt = calc_return(MU, rt, F_curr, F_prev, rft=tf.constant(0.0), delta=tf.constant(0.0))\n",
    "        # THIS DOES NOT\n",
    "        # UPDATE: THIS WORKS NOW??? NO CHANGES WERE MADE ???\n",
    "        # Rt = calc_return(MU, rt, F_curr, F_prev, tf.constant(0.0), tf.constant(0.0))\n",
    "        # ???????\n",
    "        Rt = calc_return(MU, rt, F_curr, F_prev, rft, transaction_cost)\n",
    "\n",
    "        dDt_dRt, At_new, Bt_new = calc_DSR(N, Rt, At, Bt)\n",
    "\n",
    "    # calculate derivatives.\n",
    "    dRt_dFcurr = tape.gradient(Rt, F_curr) # shape: (instances,)\n",
    "    dRt_dFprev = tape.gradient(Rt, F_prev) # shape: (instances,)\n",
    "    dF_dTheta = tape.jacobian(F_curr, F_model.trainable_variables) # shape: (instances, MODEL VAR SHAPE)\n",
    "    # a = dRt_dFcurr.numpy()\n",
    "    # b = dRt_dFprev.numpy()\n",
    "    # c = dF_dTheta.numpy()\n",
    "    # if np.isnan(a).any() or np.isnan(b).any():\n",
    "    #     print(\"NAN\")\n",
    "    # if tf.reduce_sum(dF_dTheta[0]) == 0:\n",
    "    #     a = dRt_dFcurr.numpy()\n",
    "    #     b = dRt_dFprev.numpy()\n",
    "    #     c = [d.numpy() for d in dF_dTheta]\n",
    "\n",
    "    \n",
    "\n",
    "    return Rt, F_curr, At_new, Bt_new, dDt_dRt, dRt_dFcurr, dRt_dFprev, dF_dTheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, sys\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GENERATION VARS\n",
    "import time\n",
    "\n",
    "'''\n",
    "A N0TE ON THE PARAMETERS:\n",
    "\n",
    "These parameters are appropriate for one data point per DAY.\n",
    "\n",
    "The artificial price series start at a value of 1.0, and normally scales between 0.5-2.0. \n",
    "A conversion of 1.0 to $100 is used, so the principal is $100.\n",
    "A transaction cost of 0.1 ($10) is therefore imposed (based on RBC standard order fees).\n",
    "A risk-free return of 4.00% for a $5000 3-year gov. bond is used (converted to $100 principal), \n",
    "    which converts to 0.04 / 252 (1.59E-4, $0.0159 a day).\n",
    "'''\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "if True:\n",
    "    LEARNING_RATE = 0.001\n",
    "    MAX_GRAD = 100 * LEARNING_RATE\n",
    "\n",
    "    MU = tf.constant(3.0)\n",
    "    N = tf.constant(0.01)\n",
    "    # RISKFREE_RETURN = tf.constant(1.59e-4)\n",
    "    RISKFREE_RETURN = tf.constant(0.0)\n",
    "    # TRANS_COST = tf.constant(0.01)\n",
    "    TRANS_COST = tf.constant(0.0)\n",
    "    SERIES_LENGTH = 1000\n",
    "    TRADING_DELAY = 200\n",
    "    K = 3\n",
    "    A = 0.9\n",
    "\n",
    "    EPISODES = 100\n",
    "    INSTANCES = 10\n",
    "\n",
    "    START_EPSILON = 0.5\n",
    "    END_EPSILON = 0.01\n",
    "    DECAY_PERIOD = 6000 # in number of timesteps\n",
    "    epsilon = START_EPSILON\n",
    "\n",
    "BASELINE_SERIES = gen_price_series(size=SERIES_LENGTH, k=K, a=A, instances=INSTANCES)\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "\n",
    "    # DEFINE A BUNCH OF STUFF\n",
    "    if True:\n",
    "        # generate price series\n",
    "        zt = gen_price_series(size=SERIES_LENGTH, k=K, a=A, instances=INSTANCES)\n",
    "        rt = calc_price_returns(zt)\n",
    "        Ft = np.zeros((INSTANCES, SERIES_LENGTH,), dtype=np.float32)\n",
    "\n",
    "        # breaks if init at one; MUST INIT ZERO\n",
    "        At = np.zeros((INSTANCES, SERIES_LENGTH,), dtype=np.float32)\n",
    "        Bt = np.zeros((INSTANCES, SERIES_LENGTH,), dtype=np.float32)\n",
    "        dF_dTheta_prev = None\n",
    "\n",
    "        Rt_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "        SR_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "        DSR_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "        dD_series = np.ones((INSTANCES, SERIES_LENGTH,))\n",
    "        autodiff_series = np.ones((INSTANCES, SERIES_LENGTH,))\n",
    "        bench_series = np.zeros((INSTANCES, SERIES_LENGTH,))\n",
    "        grad_series = np.zeros((SERIES_LENGTH,))\n",
    "        gradC_series = np.zeros((SERIES_LENGTH,))\n",
    "        trade_count = np.zeros((INSTANCES,))\n",
    "\n",
    "    for t in range(INPUT_SIZE, SERIES_LENGTH):\n",
    "        if (epsilon > END_EPSILON) and (t >= TRADING_DELAY):\n",
    "            epsilon -= (START_EPSILON - END_EPSILON) / DECAY_PERIOD\n",
    "        random = np.where(np.random.rand(INSTANCES) < epsilon, np.random.rand(INSTANCES) * 2 - 1, 1).astype(np.float32)\n",
    "        random = 0\n",
    "\n",
    "        Rt, Ft[:, t], At[:, t], Bt[:, t], dDt_dRt, dRt_dFcurr, dRt_dFprev, dF_dTheta = calc_grads(zt[:, t - INPUT_SIZE + 1:t + 1], rt[:, t], At[:, t-1], Bt[:, t-1], N, MU, F_model, tf.Variable(Ft[:, t-1]), RISKFREE_RETURN, TRANS_COST, random)\n",
    "        # print(Ft[0, t])\n",
    "\n",
    "        pos_changes = np.where(Ft[:, t] * Ft[:, t-1] < 0, 1, 0)\n",
    "        trade_count += pos_changes\n",
    "\n",
    "\n",
    "        '''DIAGNOSTICS. -----------------'''\n",
    "        Rt_series[:, t] = Rt\n",
    "        SR_series[:, t] = np.mean(Rt_series[:, INPUT_SIZE:t], axis=1) / np.std(Rt_series[:, INPUT_SIZE:t], axis=1)\n",
    "        DSR_series[:, t] = (Bt[:, t-1] * (Rt - At[:, t-1]) - (1/2) * (At[:, t-1] * (Rt ** 2 - Bt[:, t-1]))) / ((Bt[:, t-1] - At[:, t-1] ** 2) ** 1.5)\n",
    "        dD_series[:, t] = dDt_dRt\n",
    "        bench_series[:, t] = F_model(zt[:, -INPUT_SIZE:]).numpy().reshape(INSTANCES)\n",
    "        '''---------------------------------'''\n",
    "        \n",
    "        if np.where(SR_series[:, t] > 1.5, True, False).any():\n",
    "            a = np.mean(Rt_series[:, INPUT_SIZE:t], axis=1)\n",
    "            b = np.std(Rt_series[:, INPUT_SIZE:t], axis=1)\n",
    "            pass\n",
    "\n",
    "        '''THIS BIT APPLIES THE GRADIENTS. --------------'''\n",
    "        if (t != INPUT_SIZE) and t >= TRADING_DELAY:\n",
    "            # multiply derivatives together.\n",
    "            gradient_update = []\n",
    "\n",
    "            \n",
    "            # if t > SERIES_LENGTH - 5:\n",
    "            #     print(\"\\n\\n\\n\")\n",
    "            #     print(\"==========================================================================\")\n",
    "            #     print(\"==========================================================================\")\n",
    "\n",
    "            # print(\"===============================================================================\")\n",
    "\n",
    "            grad_mean = np.ndarray((len(dF_dTheta),), dtype=np.float32)\n",
    "            # print(dF_dTheta[-1], dF_dTheta_prev[-1], \"\\n====\\n\")\n",
    "\n",
    "            for i in range(len(dF_dTheta)):\n",
    "                total_elements = prod(dF_dTheta[i].shape)\n",
    "\n",
    "                # expand the \"scalar\" derivatives to the shape of the model variables. (jacobians)\n",
    "                dDt_dRt_exp = reshape_grad(dDt_dRt, dF_dTheta[i].shape)\n",
    "                dRt_dFcurr_exp = reshape_grad(dRt_dFcurr, dF_dTheta[i].shape)\n",
    "                dRt_dFprev_exp = reshape_grad(dRt_dFprev, dF_dTheta[i].shape)\n",
    "\n",
    "                grad = dDt_dRt_exp * (dRt_dFcurr_exp * dF_dTheta[i] + dRt_dFprev_exp * dF_dTheta_prev[i])\n",
    "                grad = tf.reduce_sum(grad, axis=0)\n",
    "                grad *= LEARNING_RATE / INSTANCES # divide by instances since the gradients are summed over all instances.\n",
    "\n",
    "                gradient_update.append(grad)\n",
    "                grad_mean[i] = tf.reduce_mean(grad)\n",
    "                \n",
    "\n",
    "                # if t > SERIES_LENGTH - 5:\n",
    "                #     tf.print(grad, output_stream=sys.stdout, summarize=-1)\n",
    "                #     print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "                #     tf.print(dDt_dRt_exp, output_stream=sys.stdout, summarize=-1)\n",
    "                #     print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "                #     tf.print(dRt_dFcurr_exp, output_stream=sys.stdout, summarize=-1)\n",
    "                #     print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "                #     tf.print(dF_dTheta[i], output_stream=sys.stdout, summarize=-1)\n",
    "                #     print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "                #     tf.print(dRt_dFprev_exp, output_stream=sys.stdout, summarize=-1)\n",
    "                #     print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "                #     tf.print(dF_dTheta_prev[i], output_stream=sys.stdout, summarize=-1)\n",
    "                #     print()\n",
    "                #     print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "                #     print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "                #     print()\n",
    "            grad_series[t] = np.mean(grad_mean)\n",
    "\n",
    "            e = [np.sum(i.numpy()) for i in dF_dTheta]\n",
    "            # print(e)\n",
    "\n",
    "            if np.abs(grad_series[t]) == 0 or sum(e) == 0:\n",
    "                a = dDt_dRt.numpy()\n",
    "                b = dRt_dFcurr.numpy()\n",
    "                c = dRt_dFprev.numpy()\n",
    "                d = [i.numpy() for i in dF_dTheta]\n",
    "                e = [i.numpy() for i in dF_dTheta_prev]\n",
    "                f = Ft[0, t]\n",
    "                apsdfhqphqpwef = 0\n",
    "\n",
    "            # gradient_update = tf.clip_by_global_norm(gradient_update, 1)[0]\n",
    "            # actually add the grads\n",
    "            vars = F_model.trainable_variables\n",
    "            grad_mean = np.ndarray((len(dF_dTheta),), dtype=np.float32)\n",
    "            for i in range(len(vars)):\n",
    "                grad = np.clip(gradient_update[i], -MAX_GRAD, MAX_GRAD)\n",
    "                grad_mean[i] = tf.reduce_mean(grad)\n",
    "                vars[i].assign_add(grad)\n",
    "            gradC_series[t] = np.mean(grad_mean)\n",
    "            '''--------------------------------------------'''\n",
    "\n",
    "        if t >= TRADING_DELAY - 1:\n",
    "            # need to continously update so that the first training step has a t-1 value.\n",
    "            dF_dTheta_prev = dF_dTheta\n",
    "\n",
    "    '''PLOT DIAGNOSTICS. -----------------'''\n",
    "    if ep % 1 == 0: \n",
    "        print(\"Episode: \", ep)\n",
    "        print(\"Mean gradient: \", np.mean(grad_series))\n",
    "        print(\"Mean clipped gradient: \", np.mean(grad_series))\n",
    "        # test performance\n",
    "        deltas, val_series = test_performance(zt, Ft)\n",
    "\n",
    "        for i in range(1):\n",
    "            print(\"# of trades: \", trade_count[i])\n",
    "            print(\"mean SR: \", np.mean(SR_series[i, INPUT_SIZE + 2:]))\n",
    "            print(\"mean DSR: \", np.mean(DSR_series[i, INPUT_SIZE + 2:]))\n",
    "\n",
    "            fig, ax = plt.subplots(3, 3, figsize=(10, 6))\n",
    "            fig.tight_layout()\n",
    "\n",
    "            ax[0, 0].plot(zt[i])\n",
    "            ax[0, 0].plot(val_series[0][i])\n",
    "            ax[0, 0].set_title(\"price\")\n",
    "\n",
    "            ax[1, 0].plot(Ft[i] * 0, color='k')\n",
    "            ax[1, 0].plot(Ft[i])\n",
    "            # ax[1, 0].set_ylim([-1, 1])\n",
    "            ax[1, 0].set_title(\"decision Ft\")\n",
    "\n",
    "            # ax[2, 0].plot(bench_series[i] * 0, color='k')\n",
    "            # ax[2, 0].plot(bench_series[i])\n",
    "            # ax[2, 0].set_ylim([-1, 1])\n",
    "            # ax[2, 0].set_title(\"benchmark Ft\")\n",
    "            ax[2, 0].plot(grad_series * 0, color='k')\n",
    "            ax[2, 0].plot(grad_series)\n",
    "            # ax[2, 0].set_ylim([-1, 1])\n",
    "            ax[2, 0].set_title(\"mean gradient\")\n",
    "\n",
    "        \n",
    "            SR_cut = SR_series.copy()\n",
    "            SR_cut[:, :TRADING_DELAY] = np.nan\n",
    "            ax[0, 1].plot(SR_cut[i])\n",
    "            ax[0, 1].set_xlim([0, SERIES_LENGTH])\n",
    "            ax[0, 1].set_title(\"sharpe ratio\")\n",
    "\n",
    "            DSR_cut = DSR_series.copy()\n",
    "            DSR_cut[:, :TRADING_DELAY] = np.nan\n",
    "            ax[1, 1].plot(DSR_series[i] * 0, color='k')\n",
    "            ax[1, 1].plot(DSR_cut[i])\n",
    "            ax[1, 1].set_xlim([0, SERIES_LENGTH])\n",
    "            ax[1, 1].set_title(\"derivative sharpe ratio\")\n",
    "\n",
    "            dD_cut = dD_series.copy()\n",
    "            dD_cut[:, :TRADING_DELAY] = np.nan\n",
    "            ax[2, 1].plot(dD_cut[i])\n",
    "            ax[2, 1].set_xlim([0, SERIES_LENGTH])\n",
    "            ax[2, 1].set_title(\"dD/dR\")\n",
    "\n",
    "            ax[0, 2].hist(grad_series, bins=100)\n",
    "            ax[0, 2].set_title(\"mean gradient\")\n",
    "            ax[1, 2].hist(SR_cut[i, int(SERIES_LENGTH / 2):], bins=100)\n",
    "            ax[1, 2].set_title(\"SR\")\n",
    "            ax[2, 2].hist(DSR_cut[i, int(SERIES_LENGTH / 2):], bins=100)\n",
    "            ax[2, 2].set_title(\"derivative SR\")\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        print(\"==========================================================================\")\n",
    "\n",
    "    '''-----------------------------------'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(Rt_series[:, INPUT_SIZE:t], axis=1)\n",
    "b = np.std(Rt_series[:, INPUT_SIZE:t], axis=1)\n",
    "c = a / b\n",
    "d = np.ndarray((INSTANCES, SERIES_LENGTH,))\n",
    "e = np.ndarray((INSTANCES, SERIES_LENGTH,))\n",
    "\n",
    "for i in range(INPUT_SIZE, SERIES_LENGTH):\n",
    "    d[:, i] = np.mean(Rt_series[:, INPUT_SIZE:i], axis=1)\n",
    "    e[:, i] = np.std(Rt_series[:, INPUT_SIZE:i], axis=1)\n",
    "\n",
    "for i in range(INSTANCES):\n",
    "    plt.plot(Rt_series[i])\n",
    "    plt.show()\n",
    "    plt.plot(d[i])\n",
    "    plt.show()\n",
    "    plt.plot(e[i])\n",
    "    plt.show()\n",
    "# plt.plot(Rt_series[0])\n",
    "# plt.show()\n",
    "# plt.plot(d[0])\n",
    "# plt.show()\n",
    "\n",
    "# np.mean(Rt_series[:, INPUT_SIZE:t], axis=1) / np.std(Rt_series[:, INPUT_SIZE:t], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(F_model, \"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_model = keras.models.load_model(\"divergent_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = F_model.trainable_variables\n",
    "for i in range(len(vars)):\n",
    "    tf.print(vars[i], output_stream=sys.stdout, summarize=-1)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# MODEL MAKING HEALTHY DECISIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(INPUT_SIZE,))\n",
    "# x = layers.Dense(20, activation='elu')(input)\n",
    "x = layers.Dense(10, activation='elu')(input)\n",
    "x = layers.Dense(1, activation='linear')(x)\n",
    "x = layers.Add()([tf.math.tanh(x), x * tf.constant(0.01)])\n",
    "A_model = keras.Model(inputs=input, outputs=x)\n",
    "vars = A_model.trainable_variables\n",
    "for i in range(len(vars)):\n",
    "    tf.print(vars[i], output_stream=sys.stdout, summarize=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = A_model.trainable_variables\n",
    "\n",
    "# vars[-1].assign_add([-20.,])\n",
    "\n",
    "for i in range(len(vars)):\n",
    "    tf.print(vars[i], output_stream=sys.stdout, summarize=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        input = np.random.uniform(0.5, 2, INPUT_SIZE).reshape(1, INPUT_SIZE)\n",
    "        # input = test\n",
    "        # input = np.array([1.3615544390032746, 0.9911959411327966, 1.860840540643432, 1.9566621882886455, 1.0216394170198488, 1.320837858199272, 0.74359758712787, 1.5175825299156631, 0.6565231677349852, 1.0815482211370182, 1.329412514047963, 0.7756110246482257, 1.9846218430759657, 0.6300063569059443, 1.6438240132480755, 1.1823738080614201, 0.7346359086574064, 1.7163596409200301, 1.5332444743534628, 1.437000217920835, 1.9873757199427518, 1.9491195850175478, 1.9819059912505954, 1.759786091395387, 0.5987470203000758, 0.68860981057173, 0.7821889719299655, 1.5952623264224697, 1.9790404298610664, 0.5405285287583309,]).reshape(1, 30)\n",
    "\n",
    "        # Fout = F_model(input)\n",
    "        Aout = A_model(input)\n",
    "\n",
    "\n",
    "    # dF = tape.gradient(Fout, F_model.trainable_variables)\n",
    "    dA = tape.gradient(Aout, A_model.trainable_variables)\n",
    "\n",
    "\n",
    "    '''print GRADS'''\n",
    "    # for i in range(len(dF)):\n",
    "    #     tf.print(dF[i], output_stream=sys.stdout, summarize=-1)\n",
    "    #     print(\"\\n\")\n",
    "    # print(\"====================================\")\n",
    "    for i in range(len(dA)):\n",
    "        tf.print(dA[i], output_stream=sys.stdout, summarize=-1)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        \n",
    "    # print(F_model(input))\n",
    "    print(A_model(input), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # if A_model(input) == 1:\n",
    "    #     break\n",
    "\n",
    "    # vars = A_model.trainable_variables\n",
    "    # for i in range(len(vars)):\n",
    "    #     vars[i].assign_add(0.001 * dA[i])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = keras.Model(inputs=A_model.inputs,\n",
    "                        outputs=[layer.output for layer in A_model.layers])\n",
    "# extractor = keras.Model(inputs=A_model.inputs, outputs=A_model.layers[-1].output)\n",
    "\n",
    "# Testing\n",
    "test = np.random.uniform(-1, 1, INPUT_SIZE).reshape(1, INPUT_SIZE)\n",
    "# test = np.array([1.657776,1.7772449,0.8355866,1.2937615,1.5042442,1.2313355 ,\n",
    "#  0.6413103,1.6870579, 1.2624699 ,1.4338475,0.5683779 ,1.5942571 ,\n",
    "#  1.8920268,1.2122076, 0.517571  ,1.1931471,0.69882214,0.7847162 ,\n",
    "#  1.7646049,1.6217262, 1.6088332 ,1.1099463,0.646141  ,1.8243487 ,\n",
    "#  1.785189 ,1.1896547, 0.60746706,1.7007248,0.5227392 ,1.684452  ] ).reshape(1, INPUT_SIZE)\n",
    "\n",
    "print(\"TEST: \", test)\n",
    "test = tf.constant(test, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # for whatever reason, calling A_model(test) instead of extractor(test) causes the gradients to be wrong,\n",
    "    # even though they should be the same.\n",
    "    tape.watch(test)\n",
    "    output = A_model(test)\n",
    "    output_e = extractor(test)\n",
    "grad = tape.jacobian(output, A_model.layers[1].trainable_variables)\n",
    "# grad_e = tape.gradient(output_e, extractor.layers[1].trainable_variables)\n",
    "\n",
    "\n",
    "features = [e.numpy() for e in extractor(test)]\n",
    "# print(features)\n",
    "elu_in = np.tensordot(A_model.trainable_variables[0], features[0][0], axes=([0], [0])) + A_model.trainable_variables[1].numpy()\n",
    "# tanh_in = np.tensordot(A_model.trainable_variables[2], features[1][0], axes=([0], [0])) + A_model.trainable_variables[3].numpy()\n",
    "print(\"Dense ELU input:\", elu_in, \"\\n\")\n",
    "tf.print(grad[1], output_stream=sys.stdout, summarize=-1)\n",
    "# tf.print(grad_e[1], output_stream=sys.stdout, summarize=-1)\n",
    "print()\n",
    "print(\"Dense ELU output:\", features[1], \"\\n\")\n",
    "print(\"Dense TANH input:\", tanh_in)\n",
    "print(\"Dense TANH output:\", features[2], \"\\n\")\n",
    "print(\"Dense A_MODEL output:\", output.numpy(), \"\\n\")\n",
    "\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(3, 2)\n",
    "b = np.arange(3)\n",
    "print(a)\n",
    "print(b)\n",
    "c = np.tensordot(a, b, axes=([0], [0]))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 3\n",
    "layer = keras.layers.Dense(1, activation='linear', input_shape=(size,))\n",
    "layer.build((size,))\n",
    "vars = layer.trainable_variables\n",
    "for i in range(len(vars)):\n",
    "    tf.print(vars[i], output_stream=sys.stdout, summarize=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inp = np.ones(size).reshape(1, size)\n",
    "inp = np.random.uniform(-1, 1, size).reshape(1, size)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    output = layer(inp)\n",
    "\n",
    "print(inp)\n",
    "intermediate = np.dot(vars[0].numpy()[:, 0], inp[0]) + vars[1].numpy()[0]\n",
    "print(\"intermediate: \", intermediate)\n",
    "print(\"out: \", output)\n",
    "\n",
    "D = tape.gradient(output, layer.trainable_variables)\n",
    "for i in range(len(D)):\n",
    "    tf.print(D[i], output_stream=sys.stdout, summarize=-1)\n",
    "    # print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inp[0] * np.power(np.cosh(np.dot(vars[0][:, 0].numpy(), inp[0]) + vars[1].numpy()[0]), -2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
